{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6040a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import streamlit as st\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n",
    "from spacy.lang.en import English\n",
    "#import en_core_web_sm\n",
    "#spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e4b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f73e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop word\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e80602",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86ba535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varibale created for words which are not included in the stopwords\n",
    "not_stopwords = (\"aren\", \"aren't\", \"couldn\", \"couldn't\", \"didn\", \"didn't\",\n",
    "                 \"doesn\", \"doesn't\", \"don\", \"don't\", \"hadn\", \"hadn't\", \"hasn\",\n",
    "                 \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"mustn\",\n",
    "                 \"mustn't\", \"no\", \"not\", \"only\", \"shouldn\", \"shouldn't\",\n",
    "                 \"should've\", \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"will\",\n",
    "                 \"wouldn\", \"wouldn't\", \"won't\", \"very\")\n",
    "stop_words_ = [words for words in stop_words if words not in not_stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7d8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional words added in the stop word list\n",
    "stop_words_.append(\"I\")\n",
    "stop_words_.append(\"the\")\n",
    "stop_words_.append(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8afc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stop word for keyword extraction\n",
    "stop_words_keywords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cfbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# special additioanl stop words added for keyword extraction\n",
    "stop_words_keywords.extend([\n",
    "    \"will\", \"always\", \"go\", \"one\", \"very\", \"good\", \"only\", \"mr\", \"lot\", \"two\",\n",
    "    \"th\", \"etc\", \"don\", \"due\", \"didn\", \"since\", \"nt\", \"ms\", \"ok\", \"almost\",\n",
    "    \"put\", \"pm\", \"hyatt\", \"grand\", \"till\", \"add\", \"let\", \"hotel\", \"able\",\n",
    "    \"per\", \"st\", \"couldn\", \"yet\", \"par\", \"hi\", \"well\", \"would\", \"I\", \"the\",\n",
    "    \"s\", \"also\", \"great\", \"get\", \"like\", \"take\", \"thank\"\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eaba9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 19:59:32.710 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/prudhvipaka/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Prediction(corpus):\n",
    "    output=[]\n",
    "    \n",
    "    #convert to string\n",
    "    review =str(corpus)\n",
    "    \n",
    "    #to handle punctuations\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    \n",
    "     # Converting Text to Lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # Spliting each words - eg ['I','was','happy']\n",
    "    review = review.split()\n",
    "\n",
    "    # Applying Lemmitization for the words eg: Argument -> Argue - Using Spacy Library\n",
    "    review = nlp(' '.join(review))\n",
    "    review = [token.lemma_ for token in review]\n",
    "\n",
    "    # Removal of stop words\n",
    "    review = [word for word in review if word not in stop_words_]\n",
    "\n",
    "    # Joining the words in sentences\n",
    "    review = ' '.join(review)\n",
    "    output.append(review)\n",
    "    \n",
    "    # TFIDF -Pickel file\n",
    "    loaded_TFIDF = load(open('vect.pkl', 'rb'))\n",
    "    #converted to number by TFIDF\n",
    "    X=pd.DataFrame((loaded_TFIDF.transform(output)).toarray())\n",
    "    \n",
    "    # PCA pickle File\n",
    "    #loaded_pca= load(open('pca.sav','rb'))\n",
    "    # apply PCA \n",
    "    #X_PCA= loaded_pca.transform(X)\n",
    "    \n",
    "    #model pickle file\n",
    "    loaded_model= load(open('lg.pkl','rb'))\n",
    "    #precition and converted to integer\n",
    "    pred = int(loaded_model.predict(X))\n",
    "    \n",
    "    if pred==1:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "def keywords(corpus):\n",
    "    output2=[]\n",
    "    \n",
    "    #convert to string\n",
    "    review =str(corpus)\n",
    "    \n",
    "    #to handle punctuations\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    \n",
    "     # Converting Text to Lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # Spliting each words - eg ['I','was','happy']\n",
    "    review = review.split()\n",
    "\n",
    "    # Applying Lemmitization for the words eg: Argument -> Argue - Using Spacy Library\n",
    "    review = nlp(' '.join(review))\n",
    "    review = [token.lemma_ for token in review]\n",
    "\n",
    "    # Removal of stop words\n",
    "    review = [word for word in review if word not in stop_words_keywords]\n",
    "\n",
    "    # Joining the words in sentences\n",
    "    review = ' '.join(review)\n",
    "    output2.append(review)\n",
    "    \n",
    "    tfidf2 = TfidfVectorizer(norm=\"l2\",analyzer='word', stop_words=stop_words_keywords,ngram_range=(1,2))\n",
    "    tfidf2_x = tfidf2.fit_transform(output2)\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_transformer.fit(tfidf2_x)\n",
    "    \n",
    "    # get feature names\n",
    "    feature_names = tfidf2.get_feature_names()\n",
    "    # generate tf-idf for the given document\n",
    "    tf_idf_vector = tfidf_transformer.transform(tfidf2.transform(output2))\n",
    "    \n",
    "    def sort_coo(coo_matrix):\n",
    "        tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "        return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    #extract only the top n, n here is 10\n",
    "    def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "            \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "            #use only topn items from vector\n",
    "            sorted_items = sorted_items[:topn]\n",
    " \n",
    "            score_vals = []\n",
    "            feature_vals = []\n",
    "    \n",
    "            # word index and corresponding tf-idf score\n",
    "            for idx, score in sorted_items:\n",
    "                #keep track of feature name and its corresponding score\n",
    "                score_vals.append(round(score, 3))\n",
    "                feature_vals.append(feature_names[idx])\n",
    " \n",
    "            #create a tuples of feature,score\n",
    "            #results = zip(feature_vals,score_vals)\n",
    "            results= feature_vals\n",
    "    \n",
    "            return pd.Series (results)\n",
    " \n",
    "    attributes=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return attributes  \n",
    "\n",
    "   # this is the main function in which we define our webpage\n",
    "def main():\n",
    "    # front end elements of the web page\n",
    "    html_temp = \"\"\" \n",
    "    <div style =\"background-color:yellow;padding:13px\"> \n",
    "    <h1 style =\"color:black;text-align:center;\">Sentiment Analysis for Hotel Review</h1> \n",
    "    </div> \n",
    "    \"\"\"\n",
    "\n",
    "    # display the front end aspect\n",
    "    st.markdown(html_temp, unsafe_allow_html=True)\n",
    "\n",
    "    # following lines create boxes in which user can enter data required to make prediction\n",
    "    # Textbox for text user is entering\n",
    "    st.subheader(\"Enter the text you'd like to analyze.\")\n",
    "    text = st.text_input('Enter text')  # text is stored in this variable\n",
    "\n",
    "    # when 'Button' is clicked, make the prediction and store it  \n",
    "    if st.button(\"Predict\"):\n",
    "        predict = Prediction(text)\n",
    "        st.success('The Sentiment of the review is {}'.format(predict))\n",
    "        \n",
    "    #if st.button(\"IMP Attributes\"):\n",
    "        st.subheader(\"Important Attributes in Reviews\")\n",
    "        imp_att=keywords(text)\n",
    "        for i in imp_att:\n",
    "            st.success(i)\n",
    "    \n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
